{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5ba86f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "891d1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/agents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00fc68f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aae15bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from order.order_team import order_team\n",
    "from semantic_kernel.contents import ChatHistory, ChatMessageContent\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "535bc7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sk_ext.planning_strategy:CreatePlan prompt: \n",
      "You are a team orchestrator that must create a plan to solve the user inquiry by using the available agents.\n",
      "Your task is to create a plan that includes only the agents suitable to help, based on their descriptions.\n",
      "The plan must be a list of agent_id values, in the order they should be executed, along with the proper instructions for each agent.\n",
      "When FEEDBACK section has content, you must consider it to tailor the plan accordingly, since this means a previous plan was not successful to meet success criteria.\n",
      "The plan must be returned as JSON, with the following structure:\n",
      "\n",
      "{\n",
      "    \"plan\": [\n",
      "        {\n",
      "            \"agent_id\": \"agent_id\",\n",
      "            \"instructions\": \"instructions\"\n",
      "        },\n",
      "        ...\n",
      "    ]\n",
      "}\n",
      "\n",
      "You MUST return the plan in the format specified above. DO NOT return anything else.\n",
      "\n",
      "# AVAILABLE AGENTS\n",
      "- agent_id: discount_agent\n",
      "    - description: This agent helps to determine if a discount is applicable based on the order details.\n",
      "\n",
      "\n",
      "\n",
      "- agent_id: order-validator\n",
      "    - description: Validates orders and checks for errors.\n",
      "\n",
      "\n",
      "\n",
      "- agent_id: substitution_agent\n",
      "    - description: \n",
      "\n",
      "\n",
      "\n",
      "- agent_id: fulfillment_agent\n",
      "    - description: An agent that helps with order fulfillment tasks.\n",
      "\n",
      "\n",
      "\n",
      "- agent_id: order-reviewer-agent\n",
      "    - description: Reviews orders and provides feedback.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# INQUIRY\n",
      "{\n",
      "  \"order\": [\n",
      "    {\n",
      "      \"sku\": \"289\",\n",
      "      \"description\": \"Sport T shirt\",\n",
      "      \"size\": \"M\",\n",
      "      \"color\": \"Red\",\n",
      "      \"quantity\": 4,\n",
      "      \"unit_price\": 10.0,\n",
      "      \"amount\": 40.0\n",
      "    },\n",
      "    {\n",
      "      \"sku\": \"953\",\n",
      "      \"description\": \"Hoodie\",\n",
      "      \"size\": \"M\",\n",
      "      \"color\": \"Black\",\n",
      "      \"quantity\": 20,\n",
      "      \"unit_price\": 25.0,\n",
      "      \"amount\": 500.0\n",
      "    },\n",
      "    {\n",
      "      \"sku\": \"5930\",\n",
      "      \"description\": \"Hoodie\",\n",
      "      \"size\": \"L\",\n",
      "      \"color\": \"Black\",\n",
      "      \"quantity\": 30,\n",
      "      \"unit_price\": 25.0,\n",
      "      \"amount\": 750.0\n",
      "    },\n",
      "    {\n",
      "      \"sku\": \"23\",\n",
      "      \"description\": \"Elephant T-shirt\",\n",
      "      \"size\": \"S\",\n",
      "      \"color\": \"White\",\n",
      "      \"quantity\": 5,\n",
      "      \"unit_price\": 100.0,\n",
      "      \"amount\": 500.0\n",
      "    }\n",
      "  ],\n",
      "  \"required\": []\n",
      "}\n",
      "\n",
      "\n",
      "# FEEDBACK\n",
      "\n",
      "\n",
      "BE SURE TO READ AGAIN THE INSTUCTIONS ABOVE BEFORE PROCEEDING.\n",
      "\n",
      "INFO:semantic_kernel.functions.kernel_function:Function CreatePlan invoking.\n",
      "INFO:httpx:HTTP Request: POST https://emea-aigbb-demos-oai.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-02-01 \"HTTP/1.1 400 BadRequest\"\n",
      "ERROR:semantic_kernel.functions.kernel_function:Function failed. Error: Error occurred while invoking function CreatePlan: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion'> service failed to complete the prompt\", BadRequestError('Error code: 400'))\n",
      "INFO:semantic_kernel.functions.kernel_function:Function completed. Duration: 0.211111s\n"
     ]
    },
    {
     "ename": "FunctionExecutionException",
     "evalue": "Error occurred while invoking function CreatePlan: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion'> service failed to complete the prompt\", BadRequestError('Error code: 400'))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:87\u001b[39m, in \u001b[36mOpenAIHandler._send_completion_request\u001b[39m\u001b[34m(self, settings)\u001b[39m\n\u001b[32m     86\u001b[39m         settings_dict.pop(\u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.chat.completions.create(**settings_dict)\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2000\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1999\u001b[39m validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2000\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2001\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2002\u001b[39m     body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2003\u001b[39m         {\n\u001b[32m   2004\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2005\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2006\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2007\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2008\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2009\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2010\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2011\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2012\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2013\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2014\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2015\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2016\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2017\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2018\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2019\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2020\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2021\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2022\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2023\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2024\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2025\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2026\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2027\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2028\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2029\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2030\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2031\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2032\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2033\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2034\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2035\u001b[39m         },\n\u001b[32m   2036\u001b[39m         completion_create_params.CompletionCreateParams,\n\u001b[32m   2037\u001b[39m     ),\n\u001b[32m   2038\u001b[39m     options=make_request_options(\n\u001b[32m   2039\u001b[39m         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2040\u001b[39m     ),\n\u001b[32m   2041\u001b[39m     cast_to=ChatCompletion,\n\u001b[32m   2042\u001b[39m     stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2043\u001b[39m     stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2044\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1767\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1764\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1765\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1766\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1767\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1461\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[39m\n\u001b[32m   1459\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1461\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m   1462\u001b[39m     cast_to=cast_to,\n\u001b[32m   1463\u001b[39m     options=options,\n\u001b[32m   1464\u001b[39m     stream=stream,\n\u001b[32m   1465\u001b[39m     stream_cls=stream_cls,\n\u001b[32m   1466\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1467\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1562\u001b[39m, in \u001b[36mAsyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1561\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1565\u001b[39m     cast_to=cast_to,\n\u001b[32m   1566\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1570\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1571\u001b[39m )\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mServiceResponseException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\semantic_kernel\\functions\\kernel_function_from_prompt.py:174\u001b[39m, in \u001b[36mKernelFunctionFromPrompt._invoke_internal\u001b[39m\u001b[34m(self, context)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     chat_message_contents = \u001b[38;5;28;01mawait\u001b[39;00m prompt_render_result.ai_service.get_chat_message_contents(\n\u001b[32m    175\u001b[39m         chat_history=chat_history,\n\u001b[32m    176\u001b[39m         settings=prompt_render_result.execution_settings,\n\u001b[32m    177\u001b[39m         **{\u001b[33m\"\u001b[39m\u001b[33mkernel\u001b[39m\u001b[33m\"\u001b[39m: context.kernel, \u001b[33m\"\u001b[39m\u001b[33marguments\u001b[39m\u001b[33m\"\u001b[39m: context.arguments},\n\u001b[32m    178\u001b[39m     )\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\chat_completion_client_base.py:134\u001b[39m, in \u001b[36mChatCompletionClientBase.get_chat_message_contents\u001b[39m\u001b[34m(self, chat_history, settings, **kwargs)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    131\u001b[39m     settings.function_choice_behavior \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m settings.function_choice_behavior.auto_invoke_kernel_functions\n\u001b[32m    133\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inner_get_chat_message_contents(chat_history, settings)\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# Auto invoke loop\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\semantic_kernel\\utils\\telemetry\\model_diagnostics\\decorators.py:112\u001b[39m, in \u001b[36mtrace_chat_completion.<locals>.inner_trace_chat_completion.<locals>.wrapper_decorator\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m are_model_diagnostics_enabled():\n\u001b[32m    111\u001b[39m     \u001b[38;5;66;03m# If model diagnostics are not enabled, just return the completion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m completion_func(*args, **kwargs)\n\u001b[32m    114\u001b[39m completion_service: \u001b[33m\"\u001b[39m\u001b[33mChatCompletionClientBase\u001b[39m\u001b[33m\"\u001b[39m = args[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_chat_completion_base.py:88\u001b[39m, in \u001b[36mOpenAIChatCompletionBase._inner_get_chat_message_contents\u001b[39m\u001b[34m(self, chat_history, settings)\u001b[39m\n\u001b[32m     86\u001b[39m settings.ai_model_id = settings.ai_model_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ai_model_id\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_request(settings)\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, ChatCompletion)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:59\u001b[39m, in \u001b[36mOpenAIHandler._send_request\u001b[39m\u001b[34m(self, settings)\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(settings, OpenAIPromptExecutionSettings)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_completion_request(settings)\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ai_model_type == OpenAIModelTypes.EMBEDDING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:99\u001b[39m, in \u001b[36mOpenAIHandler._send_completion_request\u001b[39m\u001b[34m(self, settings)\u001b[39m\n\u001b[32m     95\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ContentFilterAIException(\n\u001b[32m     96\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m service encountered a content error\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     97\u001b[39m             ex,\n\u001b[32m     98\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\n\u001b[32m    100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m service failed to complete the prompt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    101\u001b[39m         ex,\n\u001b[32m    102\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "\u001b[31mServiceResponseException\u001b[39m: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion'> service failed to complete the prompt\", BadRequestError('Error code: 400'))",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mFunctionExecutionException\u001b[39m                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m     order = file.read()\n\u001b[32m      7\u001b[39m history.add_user_message(order)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m order_team.invoke(history=history):\n\u001b[32m     10\u001b[39m     msg: ChatMessageContent = response\n\u001b[32m     11\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\semantic_kernel\\utils\\telemetry\\agent_diagnostics\\decorators.py:40\u001b[39m, in \u001b[36mtrace_agent_invocation.<locals>.wrapper_decorator\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m agent.description:\n\u001b[32m     38\u001b[39m     span.set_attribute(gen_ai_attributes.AGENT_DESCRIPTION, agent.description)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m invoke_func(*args, **kwargs):\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\tests\\../src/agents\\sk_ext\\planned_team.py:110\u001b[39m, in \u001b[36mPlannedTeam.invoke\u001b[39m\u001b[34m(self, history, arguments, kernel, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;129m@trace_agent_invocation\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m     97\u001b[39m     **kwargs: Any,\n\u001b[32m     98\u001b[39m ) -> AsyncIterable[ChatMessageContent]:\n\u001b[32m     99\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Invoke the chat history handler.\u001b[39;00m\n\u001b[32m    100\u001b[39m \n\u001b[32m    101\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    108\u001b[39m \u001b[33;03m        An async iterable of ChatMessageContent.\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inner_invoke(history, arguments, kernel, **kwargs):\n\u001b[32m    111\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\tests\\../src/agents\\sk_ext\\planned_team.py:139\u001b[39m, in \u001b[36mPlannedTeam._inner_invoke\u001b[39m\u001b[34m(self, history, arguments, kernel, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m channel.receive(local_history.messages)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_complete:\n\u001b[32m    138\u001b[39m     \u001b[38;5;66;03m# Create a plan based on the current history and feedback (if any)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     plan = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.planning_strategy.create_plan(\n\u001b[32m    140\u001b[39m         \u001b[38;5;28mself\u001b[39m.agents, local_history.messages, feedback\n\u001b[32m    141\u001b[39m     )\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m plan.plan:\n\u001b[32m    144\u001b[39m         \u001b[38;5;66;03m# Pick next agent to execute the step\u001b[39;00m\n\u001b[32m    145\u001b[39m         selected_agent = \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m    146\u001b[39m             agent \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agents \u001b[38;5;28;01mif\u001b[39;00m agent.id == step.agent_id\n\u001b[32m    147\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\tests\\../src/agents\\sk_ext\\planning_strategy.py:125\u001b[39m, in \u001b[36mDefaultPlanningStrategy.create_plan\u001b[39m\u001b[34m(self, agents, history, feedback)\u001b[39m\n\u001b[32m    121\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreatePlan prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    122\u001b[39m kfunc = KernelFunctionFromPrompt(\n\u001b[32m    123\u001b[39m     function_name=\u001b[33m\"\u001b[39m\u001b[33mCreatePlan\u001b[39m\u001b[33m\"\u001b[39m, prompt=input_prompt\n\u001b[32m    124\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m kfunc.invoke(\n\u001b[32m    126\u001b[39m     kernel=\u001b[38;5;28mself\u001b[39m.kernel,\n\u001b[32m    127\u001b[39m     arguments=arguments,\n\u001b[32m    128\u001b[39m     execution_settings=execution_settings,\n\u001b[32m    129\u001b[39m )\n\u001b[32m    130\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreatePlan: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    131\u001b[39m content = (\n\u001b[32m    132\u001b[39m     result.value[\u001b[32m0\u001b[39m].content.strip().replace(\u001b[33m\"\u001b[39m\u001b[33m```json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).replace(\u001b[33m\"\u001b[39m\u001b[33m```\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    133\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\semantic_kernel\\functions\\kernel_function.py:256\u001b[39m, in \u001b[36mKernelFunction.invoke\u001b[39m\u001b[34m(self, kernel, arguments, metadata, **kwargs)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle_exception(current_span, e, attributes)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    258\u001b[39m     duration = time.perf_counter() - starting_time_stamp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\semantic_kernel\\functions\\kernel_function.py:248\u001b[39m, in \u001b[36mKernelFunction.invoke\u001b[39m\u001b[34m(self, kernel, arguments, metadata, **kwargs)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    244\u001b[39m     stack = kernel.construct_call_stack(\n\u001b[32m    245\u001b[39m         filter_type=FilterTypes.FUNCTION_INVOCATION,\n\u001b[32m    246\u001b[39m         inner_function=\u001b[38;5;28mself\u001b[39m._invoke_internal,\n\u001b[32m    247\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m stack(function_context)\n\u001b[32m    250\u001b[39m     KernelFunctionLogMessages.log_function_invoked_success(logger, \u001b[38;5;28mself\u001b[39m.fully_qualified_name)\n\u001b[32m    251\u001b[39m     KernelFunctionLogMessages.log_function_result_value(logger, function_context.result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\semantic_kernel\\functions\\kernel_function_from_prompt.py:180\u001b[39m, in \u001b[36mKernelFunctionFromPrompt._invoke_internal\u001b[39m\u001b[34m(self, context)\u001b[39m\n\u001b[32m    174\u001b[39m     chat_message_contents = \u001b[38;5;28;01mawait\u001b[39;00m prompt_render_result.ai_service.get_chat_message_contents(\n\u001b[32m    175\u001b[39m         chat_history=chat_history,\n\u001b[32m    176\u001b[39m         settings=prompt_render_result.execution_settings,\n\u001b[32m    177\u001b[39m         **{\u001b[33m\"\u001b[39m\u001b[33mkernel\u001b[39m\u001b[33m\"\u001b[39m: context.kernel, \u001b[33m\"\u001b[39m\u001b[33marguments\u001b[39m\u001b[33m\"\u001b[39m: context.arguments},\n\u001b[32m    178\u001b[39m     )\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m FunctionExecutionException(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError occurred while invoking function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chat_message_contents:\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m FunctionExecutionException(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo completions returned while invoking function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFunctionExecutionException\u001b[39m: Error occurred while invoking function CreatePlan: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion'> service failed to complete the prompt\", BadRequestError('Error code: 400'))"
     ]
    }
   ],
   "source": [
    "history = ChatHistory()\n",
    "\n",
    "# Read order from file in /data\n",
    "with open(\"data/order1.json\", \"r\") as file:\n",
    "    order = file.read()\n",
    "\n",
    "history.add_user_message(order)\n",
    "\n",
    "async for response in order_team.invoke(history=history):\n",
    "    msg: ChatMessageContent = response\n",
    "    logger.info(f\"{msg.name}: {msg.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
