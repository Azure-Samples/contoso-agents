{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5ba86f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "891d1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/agents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00fc68f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae15bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.contents import ChatHistory, ChatMessageContent\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "535bc7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sk_ext.planning_strategy:CreatePlan prompt: \n",
      "You are a team orchestrator that must create a plan to solve the user inquiry by using the available agents.\n",
      "Your task is to create a plan that includes only the agents suitable to help, based on their descriptions.\n",
      "The plan must be a list of agent_id values, in the order they should be executed, along with the proper instructions for each agent.\n",
      "When FEEDBACK section has content, you must consider it to tailor the plan accordingly, since this means a previous plan was not successful to meet success criteria.\n",
      "The plan must be returned as JSON, with the following structure:\n",
      "\n",
      "{\n",
      "    \"plan\": [\n",
      "        {\n",
      "            \"agent_id\": \"agent_id\",\n",
      "            \"instructions\": \"instructions\"\n",
      "        },\n",
      "        ...\n",
      "    ]\n",
      "}\n",
      "\n",
      "You MUST return the plan in the format specified above. DO NOT return anything else.\n",
      "\n",
      "# AVAILABLE AGENTS\n",
      "- agent_id: pricing_agent\n",
      "    - description: This agent helps to determine if a discount is applicable based on the order details.\n",
      "\n",
      "\n",
      "\n",
      "- agent_id: validator_agent\n",
      "    - description: Validates orders and checks for errors.\n",
      "\n",
      "\n",
      "\n",
      "- agent_id: substitution_agent\n",
      "    - description: An agent that checks the availability of SKUs and suggests substitutes.\n",
      "\n",
      "\n",
      "\n",
      "- agent_id: fulfillment_agent\n",
      "    - description: An agent that helps with order fulfillment tasks. Can provide order status and build a delivery schedule.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# INQUIRY\n",
      "Please process the following order: {\n",
      "  \"customerId\": \"cust001\",\n",
      "  \"customerName\": \"Acme Corporation\",\n",
      "  \"order\": [\n",
      "    {\n",
      "      \"sku\": \"SKU-A100\",\n",
      "      \"description\": \"Sport T shirt\",\n",
      "      \"size\": \"M\",\n",
      "      \"color\": \"Red\",\n",
      "      \"quantity\": 4,\n",
      "      \"unit_price\": 10.0\n",
      "    },\n",
      "    {\n",
      "      \"sku\": \"SKU-A102\",\n",
      "      \"description\": \"Hoodie\",\n",
      "      \"size\": \"M\",\n",
      "      \"color\": \"Black\",\n",
      "      \"quantity\": 20,\n",
      "      \"unit_price\": 25.0\n",
      "    },\n",
      "    {\n",
      "      \"sku\": \"SKU-A103\",\n",
      "      \"description\": \"Hoodie\",\n",
      "      \"size\": \"L\",\n",
      "      \"color\": \"Black\",\n",
      "      \"quantity\": 30,\n",
      "      \"unit_price\": 25.0\n",
      "    },\n",
      "    {\n",
      "      \"sku\": \"SKU-A104\",\n",
      "      \"description\": \"Elephant T-shirt\",\n",
      "      \"size\": \"S\",\n",
      "      \"color\": \"White\",\n",
      "      \"quantity\": 5,\n",
      "      \"unit_price\": 100.0\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "\n",
      "# FEEDBACK\n",
      "\n",
      "\n",
      "BE SURE TO READ AGAIN THE INSTUCTIONS ABOVE BEFORE PROCEEDING.\n",
      "\n",
      "INFO:semantic_kernel.functions.kernel_function:Function CreatePlan invoking.\n",
      "INFO:httpx:HTTP Request: POST https://emea-aigbb-demos-oai.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:semantic_kernel.connectors.ai.open_ai.services.open_ai_handler:OpenAI usage: CompletionUsage(completion_tokens=142, prompt_tokens=548, total_tokens=690, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "INFO:semantic_kernel.functions.kernel_function:Function CreatePlan succeeded.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function completed. Duration: 2.376631s\n",
      "INFO:sk_ext.planning_strategy:CreatePlan: {\n",
      "    \"plan\": [\n",
      "        {\n",
      "            \"agent_id\": \"validator_agent\",\n",
      "            \"instructions\": \"Validate the order for any errors and ensure all SKUs are correct.\"\n",
      "        },\n",
      "        {\n",
      "            \"agent_id\": \"pricing_agent\",\n",
      "            \"instructions\": \"Determine if any discounts are applicable based on the order details.\"\n",
      "        },\n",
      "        {\n",
      "            \"agent_id\": \"substitution_agent\",\n",
      "            \"instructions\": \"Check the availability of each SKU and suggest substitutes if any are out of stock.\"\n",
      "        },\n",
      "        {\n",
      "            \"agent_id\": \"fulfillment_agent\",\n",
      "            \"instructions\": \"Process the order for fulfillment, provide order status, and build a delivery schedule.\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "INFO:httpx:HTTP Request: POST https://emea-aigbb-demos-oai.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:semantic_kernel.connectors.ai.open_ai.services.open_ai_handler:OpenAI usage: CompletionUsage(completion_tokens=35, prompt_tokens=538, total_tokens=573, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "INFO:semantic_kernel.connectors.ai.chat_completion_client_base:processing 1 tool calls in parallel.\n",
      "INFO:semantic_kernel.kernel:Calling ValidationPlugin-validate_skus function with args: {\"sku_list\":[\"SKU-A100\",\"SKU-A102\",\"SKU-A103\",\"SKU-A104\"]}\n",
      "INFO:semantic_kernel.functions.kernel_function:Function ValidationPlugin-validate_skus invoking.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function ValidationPlugin-validate_skus succeeded.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function completed. Duration: 0.002677s\n",
      "INFO:httpx:HTTP Request: POST https://emea-aigbb-demos-oai.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:semantic_kernel.connectors.ai.open_ai.services.open_ai_handler:OpenAI usage: CompletionUsage(completion_tokens=4, prompt_tokens=585, total_tokens=589, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "INFO:__main__:OrderValidator: Valid order\n",
      "INFO:httpx:HTTP Request: POST https://emea-aigbb-demos-oai.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:semantic_kernel.connectors.ai.open_ai.services.open_ai_handler:OpenAI usage: CompletionUsage(completion_tokens=237, prompt_tokens=456, total_tokens=693, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "INFO:semantic_kernel.connectors.ai.chat_completion_client_base:processing 8 tool calls in parallel.\n",
      "INFO:semantic_kernel.kernel:Calling PricingAgentPlugin-check_discount function with args: {\"sku\": \"SKU-A100\", \"quantity\": 4}\n",
      "INFO:semantic_kernel.functions.kernel_function:Function PricingAgentPlugin-check_discount invoking.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function PricingAgentPlugin-check_discount succeeded.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function completed. Duration: 0.001398s\n",
      "INFO:semantic_kernel.kernel:Calling PricingAgentPlugin-check_discount function with args: {\"sku\": \"SKU-A102\", \"quantity\": 20}\n",
      "INFO:semantic_kernel.functions.kernel_function:Function PricingAgentPlugin-check_discount invoking.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function PricingAgentPlugin-check_discount succeeded.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function completed. Duration: 0.001461s\n",
      "INFO:semantic_kernel.kernel:Calling PricingAgentPlugin-check_discount function with args: {\"sku\": \"SKU-A103\", \"quantity\": 30}\n",
      "INFO:semantic_kernel.functions.kernel_function:Function PricingAgentPlugin-check_discount invoking.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function PricingAgentPlugin-check_discount succeeded.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function completed. Duration: 0.001341s\n",
      "INFO:semantic_kernel.kernel:Calling PricingAgentPlugin-check_discount function with args: {\"sku\": \"SKU-A104\", \"quantity\": 5}\n",
      "INFO:semantic_kernel.functions.kernel_function:Function PricingAgentPlugin-check_discount invoking.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function PricingAgentPlugin-check_discount succeeded.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function completed. Duration: 0.002868s\n",
      "INFO:semantic_kernel.kernel:Calling PricingAgentPlugin-check_customer_pricelist function with args: {\"sku\": \"SKU-A100\", \"customer_id\": \"cust001\"}\n",
      "INFO:semantic_kernel.functions.kernel_function:Function PricingAgentPlugin-check_customer_pricelist invoking.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function PricingAgentPlugin-check_customer_pricelist succeeded.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function completed. Duration: 0.002034s\n",
      "INFO:semantic_kernel.kernel:Calling PricingAgentPlugin-check_customer_pricelist function with args: {\"sku\": \"SKU-A102\", \"customer_id\": \"cust001\"}\n",
      "INFO:semantic_kernel.functions.kernel_function:Function PricingAgentPlugin-check_customer_pricelist invoking.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function PricingAgentPlugin-check_customer_pricelist succeeded.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function completed. Duration: 0.001606s\n",
      "INFO:semantic_kernel.kernel:Calling PricingAgentPlugin-check_customer_pricelist function with args: {\"sku\": \"SKU-A103\", \"customer_id\": \"cust001\"}\n",
      "INFO:semantic_kernel.functions.kernel_function:Function PricingAgentPlugin-check_customer_pricelist invoking.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function PricingAgentPlugin-check_customer_pricelist succeeded.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function completed. Duration: 0.002021s\n",
      "INFO:semantic_kernel.kernel:Calling PricingAgentPlugin-check_customer_pricelist function with args: {\"sku\": \"SKU-A104\", \"customer_id\": \"cust001\"}\n",
      "INFO:semantic_kernel.functions.kernel_function:Function PricingAgentPlugin-check_customer_pricelist invoking.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function PricingAgentPlugin-check_customer_pricelist succeeded.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function completed. Duration: 0.002231s\n",
      "INFO:httpx:HTTP Request: POST https://emea-aigbb-demos-oai.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:semantic_kernel.connectors.ai.open_ai.services.open_ai_handler:OpenAI usage: CompletionUsage(completion_tokens=188, prompt_tokens=734, total_tokens=922, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "INFO:__main__:PricingAgent: Here are the results for the order from Acme Corporation:\n",
      "\n",
      "### Discounts Applicable:\n",
      "1. **SKU-A100 (Sport T-shirt)**: **10% discount** for a quantity of 4.\n",
      "2. **SKU-A102 (Hoodie)**: **20% discount** for a quantity of 20.\n",
      "3. **SKU-A103 (Hoodie)**: **No discount** for a quantity of 30.\n",
      "4. **SKU-A104 (Elephant T-shirt)**: **15% discount** for a quantity of 5.\n",
      "\n",
      "### Custom Pricing:\n",
      "- **SKU-A100**: No custom price assigned.\n",
      "- **SKU-A102**: No custom price assigned.\n",
      "- **SKU-A103**: No custom price assigned.\n",
      "- **SKU-A104**: No custom price assigned.\n",
      "\n",
      "If you would like to proceed with the order considering these discounts or have any questions, please let me know!\n",
      "INFO:httpx:HTTP Request: POST https://emea-aigbb-demos-oai.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:semantic_kernel.connectors.ai.open_ai.services.open_ai_handler:OpenAI usage: CompletionUsage(completion_tokens=38, prompt_tokens=959, total_tokens=997, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "INFO:semantic_kernel.connectors.ai.chat_completion_client_base:processing 1 tool calls in parallel.\n",
      "INFO:semantic_kernel.kernel:Calling SubstitutionAgentPlugin-check_availability function with args: {\"skus_to_check\":[\"SKU-A100\",\"SKU-A102\",\"SKU-A103\",\"SKU-A104\"]}\n",
      "INFO:semantic_kernel.functions.kernel_function:Function SubstitutionAgentPlugin-check_availability invoking.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function SubstitutionAgentPlugin-check_availability succeeded.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function completed. Duration: 0.001776s\n",
      "INFO:httpx:HTTP Request: POST https://emea-aigbb-demos-oai.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:semantic_kernel.connectors.ai.open_ai.services.open_ai_handler:OpenAI usage: CompletionUsage(completion_tokens=27, prompt_tokens=1041, total_tokens=1068, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "INFO:semantic_kernel.connectors.ai.chat_completion_client_base:processing 1 tool calls in parallel.\n",
      "INFO:semantic_kernel.kernel:Calling SubstitutionAgentPlugin-get_substitutes function with args: {\"skus_to_check\":[\"SKU-A102\"]}\n",
      "INFO:semantic_kernel.functions.kernel_function:Function SubstitutionAgentPlugin-get_substitutes invoking.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function SubstitutionAgentPlugin-get_substitutes succeeded.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function completed. Duration: 0.002216s\n",
      "INFO:httpx:HTTP Request: POST https://emea-aigbb-demos-oai.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:semantic_kernel.connectors.ai.open_ai.services.open_ai_handler:OpenAI usage: CompletionUsage(completion_tokens=161, prompt_tokens=1082, total_tokens=1243, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "INFO:__main__:SubstitutionAgent: ### Availability Check Results:\n",
      "1. **SKU-A100 (Sport T-shirt)**: **Available** (100 in stock)\n",
      "2. **SKU-A102 (Hoodie)**: **Not Available** (0 in stock)\n",
      "3. **SKU-A103 (Hoodie)**: **Available** (5 in stock)\n",
      "4. **SKU-A104 (Elephant T-shirt)**: **Available** (20 in stock)\n",
      "\n",
      "### Substitutes:\n",
      "Unfortunately, there are no substitutes available for **SKU-A102 (Hoodie)**.\n",
      "\n",
      "### Actions:\n",
      "- You can choose to remove **SKU-A102** from the order or adjust the quantity of available items **SKU-A103** if it meets your requirement.\n",
      "- Please let me know how you'd like to proceed with your order!\n",
      "INFO:httpx:HTTP Request: POST https://emea-aigbb-demos-oai.openai.azure.com/openai/deployments/o3-mini/chat/completions?api-version=2024-08-01-preview \"HTTP/1.1 400 BadRequest\"\n"
     ]
    },
    {
     "ename": "ServiceResponseException",
     "evalue": "(\"<class 'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion'> service failed to complete the prompt\", BadRequestError(\"Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Model {modelName} is enabled only for api versions 2024-12-01-preview and later'}}\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:87\u001b[39m, in \u001b[36mOpenAIHandler._send_completion_request\u001b[39m\u001b[34m(self, settings)\u001b[39m\n\u001b[32m     86\u001b[39m         settings_dict.pop(\u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.chat.completions.create(**settings_dict)\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2000\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1999\u001b[39m validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2000\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2001\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2002\u001b[39m     body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2003\u001b[39m         {\n\u001b[32m   2004\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2005\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2006\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2007\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2008\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2009\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2010\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2011\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2012\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2013\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2014\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2015\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2016\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2017\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2018\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2019\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2020\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2021\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2022\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2023\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2024\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2025\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2026\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2027\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2028\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2029\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2030\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2031\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2032\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2033\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2034\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2035\u001b[39m         },\n\u001b[32m   2036\u001b[39m         completion_create_params.CompletionCreateParams,\n\u001b[32m   2037\u001b[39m     ),\n\u001b[32m   2038\u001b[39m     options=make_request_options(\n\u001b[32m   2039\u001b[39m         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2040\u001b[39m     ),\n\u001b[32m   2041\u001b[39m     cast_to=ChatCompletion,\n\u001b[32m   2042\u001b[39m     stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2043\u001b[39m     stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2044\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1767\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1764\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1765\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1766\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1767\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1461\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[39m\n\u001b[32m   1459\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1461\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m   1462\u001b[39m     cast_to=cast_to,\n\u001b[32m   1463\u001b[39m     options=options,\n\u001b[32m   1464\u001b[39m     stream=stream,\n\u001b[32m   1465\u001b[39m     stream_cls=stream_cls,\n\u001b[32m   1466\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1467\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1562\u001b[39m, in \u001b[36mAsyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1561\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1565\u001b[39m     cast_to=cast_to,\n\u001b[32m   1566\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1570\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1571\u001b[39m )\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Model {modelName} is enabled only for api versions 2024-12-01-preview and later'}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mServiceResponseException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m     order = file.read()\n\u001b[32m      8\u001b[39m history.add_user_message(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease process the following order: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00morder\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m order_team.invoke(history=history):\n\u001b[32m     11\u001b[39m     msg: ChatMessageContent = response\n\u001b[32m     12\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\semantic_kernel\\utils\\telemetry\\agent_diagnostics\\decorators.py:40\u001b[39m, in \u001b[36mtrace_agent_invocation.<locals>.wrapper_decorator\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m agent.description:\n\u001b[32m     38\u001b[39m     span.set_attribute(gen_ai_attributes.AGENT_DESCRIPTION, agent.description)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m invoke_func(*args, **kwargs):\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\tests\\../src/agents\\sk_ext\\planned_team.py:110\u001b[39m, in \u001b[36mPlannedTeam.invoke\u001b[39m\u001b[34m(self, history, arguments, kernel, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;129m@trace_agent_invocation\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m     97\u001b[39m     **kwargs: Any,\n\u001b[32m     98\u001b[39m ) -> AsyncIterable[ChatMessageContent]:\n\u001b[32m     99\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Invoke the chat history handler.\u001b[39;00m\n\u001b[32m    100\u001b[39m \n\u001b[32m    101\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    108\u001b[39m \u001b[33;03m        An async iterable of ChatMessageContent.\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inner_invoke(history, arguments, kernel, **kwargs):\n\u001b[32m    111\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\tests\\../src/agents\\sk_ext\\planned_team.py:158\u001b[39m, in \u001b[36mPlannedTeam._inner_invoke\u001b[39m\u001b[34m(self, history, arguments, kernel, **kwargs)\u001b[39m\n\u001b[32m    149\u001b[39m local_history.add_message(\n\u001b[32m    150\u001b[39m     ChatMessageContent(\n\u001b[32m    151\u001b[39m         role=AuthorRole.ASSISTANT,\n\u001b[32m   (...)\u001b[39m\u001b[32m    154\u001b[39m     )\n\u001b[32m    155\u001b[39m )\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# Then invoke the agent\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m is_visible, message \u001b[38;5;129;01min\u001b[39;00m channel.invoke(selected_agent):\n\u001b[32m    159\u001b[39m     local_history.add_message(message)\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_visible \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fork_history:\n\u001b[32m    162\u001b[39m         \u001b[38;5;66;03m# If we are not forking history, we can yield the message\u001b[39;00m\n\u001b[32m    163\u001b[39m         \u001b[38;5;66;03m# This prevents forked message to appear in the main history\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\semantic_kernel\\agents\\channels\\chat_history_channel.py:63\u001b[39m, in \u001b[36mChatHistoryChannel.invoke\u001b[39m\u001b[34m(self, agent, **kwargs)\u001b[39m\n\u001b[32m     60\u001b[39m mutated_history = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m     61\u001b[39m message_queue: Deque[ChatMessageContent] = deque()\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response_message \u001b[38;5;129;01min\u001b[39;00m agent.invoke(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Capture all messages that have been included in the mutated history.\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m message_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(message_count, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.messages)):\n\u001b[32m     66\u001b[39m         mutated_message = \u001b[38;5;28mself\u001b[39m.messages[message_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\semantic_kernel\\utils\\telemetry\\agent_diagnostics\\decorators.py:40\u001b[39m, in \u001b[36mtrace_agent_invocation.<locals>.wrapper_decorator\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m agent.description:\n\u001b[32m     38\u001b[39m     span.set_attribute(gen_ai_attributes.AGENT_DESCRIPTION, agent.description)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m invoke_func(*args, **kwargs):\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\semantic_kernel\\agents\\chat_completion\\chat_completion_agent.py:189\u001b[39m, in \u001b[36mChatCompletionAgent.invoke\u001b[39m\u001b[34m(self, history, arguments, kernel, **kwargs)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;129m@trace_agent_invocation\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    176\u001b[39m     **kwargs: Any,\n\u001b[32m    177\u001b[39m ) -> AsyncIterable[ChatMessageContent]:\n\u001b[32m    178\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Invoke the chat history handler.\u001b[39;00m\n\u001b[32m    179\u001b[39m \n\u001b[32m    180\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    187\u001b[39m \u001b[33;03m        An async iterable of ChatMessageContent.\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inner_invoke(history, arguments, kernel, **kwargs):\n\u001b[32m    190\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\semantic_kernel\\agents\\chat_completion\\chat_completion_agent.py:319\u001b[39m, in \u001b[36mChatCompletionAgent._inner_invoke\u001b[39m\u001b[34m(self, history, arguments, kernel, **kwargs)\u001b[39m\n\u001b[32m    315\u001b[39m message_count_before_completion = \u001b[38;5;28mlen\u001b[39m(agent_chat_history)\n\u001b[32m    317\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Invoking \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(chat_completion_service).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m responses = \u001b[38;5;28;01mawait\u001b[39;00m chat_completion_service.get_chat_message_contents(\n\u001b[32m    320\u001b[39m     chat_history=agent_chat_history,\n\u001b[32m    321\u001b[39m     settings=settings,\n\u001b[32m    322\u001b[39m     kernel=kernel,\n\u001b[32m    323\u001b[39m     arguments=arguments,\n\u001b[32m    324\u001b[39m )\n\u001b[32m    326\u001b[39m logger.debug(\n\u001b[32m    327\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Invoked \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(chat_completion_service).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    328\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwith message count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage_count_before_completion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    329\u001b[39m )\n\u001b[32m    331\u001b[39m \u001b[38;5;28mself\u001b[39m._capture_mutated_messages(history, agent_chat_history, message_count_before_completion)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\chat_completion_client_base.py:139\u001b[39m, in \u001b[36mChatCompletionClientBase.get_chat_message_contents\u001b[39m\u001b[34m(self, chat_history, settings, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m use_span(\u001b[38;5;28mself\u001b[39m._start_auto_function_invocation_activity(kernel, settings), end_on_exit=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m _:\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m request_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(settings.function_choice_behavior.maximum_auto_invoke_attempts):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m         completions = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inner_get_chat_message_contents(chat_history, settings)\n\u001b[32m    140\u001b[39m         \u001b[38;5;66;03m# Get the function call contents from the chat message. There is only one chat message,\u001b[39;00m\n\u001b[32m    141\u001b[39m         \u001b[38;5;66;03m# which should be checked in the `_verify_function_choice_settings` method.\u001b[39;00m\n\u001b[32m    142\u001b[39m         function_calls = [item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m completions[\u001b[32m0\u001b[39m].items \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, FunctionCallContent)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\semantic_kernel\\utils\\telemetry\\model_diagnostics\\decorators.py:112\u001b[39m, in \u001b[36mtrace_chat_completion.<locals>.inner_trace_chat_completion.<locals>.wrapper_decorator\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(completion_func)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper_decorator\u001b[39m(*args: Any, **kwargs: Any) -> \u001b[38;5;28mlist\u001b[39m[ChatMessageContent]:\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m are_model_diagnostics_enabled():\n\u001b[32m    111\u001b[39m         \u001b[38;5;66;03m# If model diagnostics are not enabled, just return the completion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m completion_func(*args, **kwargs)\n\u001b[32m    114\u001b[39m     completion_service: \u001b[33m\"\u001b[39m\u001b[33mChatCompletionClientBase\u001b[39m\u001b[33m\"\u001b[39m = args[\u001b[32m0\u001b[39m]\n\u001b[32m    115\u001b[39m     chat_history: ChatHistory = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mchat_history\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[32m1\u001b[39m]  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_chat_completion_base.py:88\u001b[39m, in \u001b[36mOpenAIChatCompletionBase._inner_get_chat_message_contents\u001b[39m\u001b[34m(self, chat_history, settings)\u001b[39m\n\u001b[32m     85\u001b[39m settings.messages = \u001b[38;5;28mself\u001b[39m._prepare_chat_history_for_request(chat_history)\n\u001b[32m     86\u001b[39m settings.ai_model_id = settings.ai_model_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ai_model_id\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_request(settings)\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, ChatCompletion)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n\u001b[32m     90\u001b[39m response_metadata = \u001b[38;5;28mself\u001b[39m._get_metadata_from_chat_response(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:59\u001b[39m, in \u001b[36mOpenAIHandler._send_request\u001b[39m\u001b[34m(self, settings)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ai_model_type == OpenAIModelTypes.TEXT \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ai_model_type == OpenAIModelTypes.CHAT:\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(settings, OpenAIPromptExecutionSettings)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_completion_request(settings)\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ai_model_type == OpenAIModelTypes.EMBEDDING:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(settings, OpenAIEmbeddingPromptExecutionSettings)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ricchi\\Repos\\contoso-agents\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\open_ai\\services\\open_ai_handler.py:99\u001b[39m, in \u001b[36mOpenAIHandler._send_completion_request\u001b[39m\u001b[34m(self, settings)\u001b[39m\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ex.code == \u001b[33m\"\u001b[39m\u001b[33mcontent_filter\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     95\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ContentFilterAIException(\n\u001b[32m     96\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m service encountered a content error\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     97\u001b[39m             ex,\n\u001b[32m     98\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\n\u001b[32m    100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m service failed to complete the prompt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    101\u001b[39m         ex,\n\u001b[32m    102\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\n\u001b[32m    105\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m service failed to complete the prompt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    106\u001b[39m         ex,\n\u001b[32m    107\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n",
      "\u001b[31mServiceResponseException\u001b[39m: (\"<class 'semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion.AzureChatCompletion'> service failed to complete the prompt\", BadRequestError(\"Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Model {modelName} is enabled only for api versions 2024-12-01-preview and later'}}\"))"
     ]
    }
   ],
   "source": [
    "from order.order_team import order_team\n",
    "history = ChatHistory()\n",
    "\n",
    "# Read order from file in /data\n",
    "with open(\"data/order1.json\", \"r\") as file:\n",
    "    order = file.read()\n",
    "\n",
    "history.add_user_message(f\"Please process the following order: {order}\")\n",
    "\n",
    "async for response in order_team.invoke(history=history):\n",
    "    msg: ChatMessageContent = response\n",
    "    logger.info(f\"{msg.name}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dccfab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in history.messages:\n",
    "    print(f\"{m.name}: {m.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
